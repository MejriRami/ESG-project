{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c31804-1002-4490-905a-625c12fa0055",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380731b9-b002-4b65-a3d5-8ac59c9b48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas nltk scikit-learn gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b18a6f-c2bc-4a09-bb8f-08b4ce5eb067",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6d116-cf4f-44f0-918a-2c54f98c57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96528a2b-f173-4145-8382-4a7e1d8bd322",
   "metadata": {},
   "source": [
    " Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea131e-b53f-4a8e-be99-f62d16ffddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel dataset\n",
    "df = pd.read_excel('your_dataset.xlsx')\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35663e5-272d-4b6f-bb85-a9ea0a69bf81",
   "metadata": {},
   "source": [
    " Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452f4dc-bab9-4cbb-8e28-76b356168a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Handle missing values (remove rows with missing text)\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f7321-b6b3-41f1-ae04-911ef1f9a856",
   "metadata": {},
   "source": [
    "Text Normalization (Lowercase, Tokenization, Stop Words, Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0d119-d85e-4ac5-b359-3e3e496b7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply normalization to the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Preview normalized data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd0804-7d05-4bec-ac8f-f14043ab6b7e",
   "metadata": {},
   "source": [
    "Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00b6ab-e7b0-41b4-be27-48e9ad9aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training Set: {len(train_df)} samples')\n",
    "print(f'Validation Set: {len(val_df)} samples')\n",
    "print(f'Test Set: {len(test_df)} samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3039169-abd6-4ee8-ac2d-f131e5c24776",
   "metadata": {},
   "source": [
    "GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f5e66-65ee-4629-ba81-08b625b7bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = api.load('glove-wiki-gigaword-100')  # 100-dimensional embeddings\n",
    "\n",
    "# Function to get GloVe vectors for each word in the text\n",
    "def get_glove_embeddings(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [glove[word] for word in words if word in glove]\n",
    "    if len(word_vectors) > 0:\n",
    "        return sum(word_vectors) / len(word_vectors)  # Return average of word vectors\n",
    "    else:\n",
    "        return [0] * 100  # Return zero vector if no words in GloVe\n",
    "\n",
    "# Apply GloVe embeddings to 'text' column\n",
    "train_df['glove_vectors'] = train_df['text'].apply(lambda x: get_glove_embeddings(x))\n",
    "val_df['glove_vectors'] = val_df['text'].apply(lambda x: get_glove_embeddings(x))\n",
    "test_df['glove_vectors'] = test_df['text'].apply(lambda x: get_glove_embeddings(x))\n",
    "\n",
    "# Check the new glove_vectors column\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c266130-e2aa-4967-9d55-9505ebd6cec1",
   "metadata": {},
   "source": [
    "Save the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58abf2-0486-4aed-aa80-3dc96f586368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to CSV\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
